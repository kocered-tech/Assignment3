---
title: "Erdem_NN1"
format: html
editor: visual
---

## Install libraries

Uncomment to install libraries

```{r}
#install.packages("drat", repos="https://cran.rstudio.com")
#drat:::addRepo("dmlc")
#install.packages("xgboost")
#install.packages("caret)
#install.packages("xgboost)
#install.packages("corrplot")
#install.packages("glmnet")
```

```{r}
library(tidyverse)
library(keras)
library(ggplot2)
library(xgboost)
library(fastDummies)
library(dplyr)
library(caret)
library(corrplot)
library(glmnet)
```

## Data Preparation

```{r}
train_data <- readRDS("train.rds")
train_data
test_data <- readRDS("test.rds")
```

## EDA

```{r}
train_data |> ggplot(aes(x=score)) +
  geom_histogram()
```

## Convert factors to dummy variables

```{r}
train_data_dummy <- dummy_cols(train_data,remove_first_dummy=TRUE,remove_selected_columns=TRUE)
train_data_dummy
```

```{r}
corrplot(cor(train_data_dummy)[1:20,1:20], method="color")
corrplot(cor(train_data_dummy)[20:40,20:40], method="color")
```

## Sample train and test data

```{r}
set.seed(40)
sample <- sample(c(TRUE, FALSE), nrow(train_data_dummy), replace=TRUE, prob=c(0.7,0.3))
train  <- train_data_dummy[sample, ]
test   <- train_data_dummy[!sample, ]
```

```{r}
train |> ggplot(aes(x=score)) +
  geom_histogram()
```

```{r}
test |> ggplot(aes(x=score)) +
  geom_histogram()
```

## Train, test split

```{r}
X_train <- train[, -which(names(train) == "score")]
y_train <- data.frame(train$score)
X_test <- test[, -which(names(train) == "score")]
y_test <- test$score
```

## Models

## KNN

```{r}
knnmodel = knnreg(X_train, y_train$train.score,k=20)
pred_y_knn = predict(knnmodel, X_test)

mse = mean((y_test - pred_y_knn)^2)
mae = caret::MAE(y_test, pred_y_knn)
rmse = caret::RMSE(y_test, pred_y_knn)

cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmse)
```

```{r}
ggplot(data=NULL, aes(x=y_test,y=pred_y_knn)) +
  geom_point()+
  geom_abline(intercept = 0, slope = 1, color = "blue") +
  labs(x = "Observed", y = "Predicted") +
  ggtitle("Observed vs. Predicted") +
  xlim(c(-2,2)) +
  ylim(c(-2,2))
```

## XGBoost

```{r}
xgb_train = xgb.DMatrix(data = as.matrix(X_train), label = as.matrix(y_train))
xgb_test = xgb.DMatrix(data = as.matrix(X_test), label = as.matrix(y_test))

watchlist = list(train=xgb_train, test=xgb_test)
xgb_model = xgb.train(data = xgb_train, max.depth = 8, watchlist=watchlist, nrounds = 100)
```

```{r}
model_xgboost = xgboost(data = xgb_train, max.depth = 5, nrounds = 40, verbose = 0)
```

## XGBoost model performance

```{r}
pred_y_xgboost <- predict(model_xgboost,xgb_test)

# Calculate the Mean Squared Error (MSE)
mean((pred_y_xgboost - y_test)^2)
```

```{r}
ggplot(data=NULL, aes(x=y_test,y=pred_y_xgboost)) +
  geom_point()+
  geom_abline(intercept = 0, slope = 1, color = "blue") +
  labs(x = "Observed", y = "Predicted") +
  ggtitle("Observed vs. Predicted") +
  xlim(c(-2,2)) +
  ylim(c(-2,2))
```

## Neural Network

```{r}
multinom <- keras_model_sequential() %>%
    layer_dense(16, activation = 'tanh') %>%
    layer_dense(8, activation = 'sigmoid') %>%
    layer_dense(1)

multinom %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adam()
)

```

```         
```

```{r}
pl <- multinom %>% fit(as.matrix(X_train), 
      as.matrix(y_train), 
      epochs = 60, 
      validation_split = 0.1,       
      verbose = 1)
```

```{r}
pred_y_nn <- predict(multinom, as.matrix(X_test))

# Calculate the Mean Squared Error (MSE)
mean((as.matrix(y_test) - pred_y_nn)^2)
```

```{r}
ggplot(data=NULL, aes(x=y_test,y=pred_y_nn)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue") +
  labs(x = "Observed", y = "Predicted") +
  ggtitle("Observed vs. Predicted") +
  xlim(c(-2,2)) +
  ylim(c(-2,2))
```

```{r}
predict_linreg = lm(score~.,data=train)
#summary(predict_linreg)
pred_y_linreg <- predict(predict_linreg,newdata=test)

# Calculate the Mean Squared Error (MSE)
mean((pred_y_linreg - y_test)^2)
```

```{r}
ggplot(data=NULL, aes(x=y_test,y=pred_y_linreg)) +
  geom_point()+
  geom_abline(intercept = 0, slope = 1, color = "blue") +
  labs(x = "Observed", y = "Predicted") +
  ggtitle("Observed vs. Predicted") +
  xlim(c(-2,2)) +
  ylim(c(-2,2))
```

## Lasso Regression

```{r}
#perform k-fold cross-validation to find optimal lambda value
original_weights <- rep(1, nrow(X_train))  # Initialize all weights to 1
score_threshold <- 1  # Define the threshold

# Double the weights for observations with a score larger than 1
custom_weights <- ifelse(y > score_threshold, 2 * original_weights, original_weights)

cv_model <- cv.glmnet(data.matrix(X_train),weights=custom_weights, data.matrix(y_train), alpha = 1, type.measure="mse")

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_model <- glmnet(data.matrix(X_train), data.matrix(y_train), alpha = 1,type.measure="mse", lambda = best_lambda)
coef(best_model)
```

```{r}
plot(cv_model) 
```

```{r}
pred_y_lasso <- predict(best_model, s = best_lambda, newx = data.matrix(X_test))
```

```{r}
lasso <- mean((y_test - pred_y_lasso)^2)
knn <- mean((y_test - pred_y_knn)^2)
xgboost <- mean((y_test - pred_y_xgboost)^2)
lin <- mean((y_test - pred_y_linreg)^2)
nn <- mean((as.matrix(y_test) - pred_y_nn)^2)

average <- (pred_y_nn+pred_y_lasso)/2
average_mse <- mean((as.matrix(y_test) - average)^2)

cat("knn: ", knn, 
    "xgboost: ", xgboost,
    "linear: ", lin,
    "lasso: ", lasso,
    "nn: ",nn,
    "average: ",average_mse)


```

```{r}
mean_y_train <- mean(as.matrix(y_train))
mean_y_train
```

```{r}
mean((as.matrix(y_test) - mean_y_train)^2)
```

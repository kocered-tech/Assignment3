layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 50,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
squared_diff <- (y_test - y_pred)^2
# Calculate the Mean Squared Error (MSE)
mse <- mean(squared_diff)
y_pred <- predict(multinom, as.matrix(X_test))
squared_diff <- (y_test - y_pred)^2
# Calculate the Mean Squared Error (MSE)
mse <- mean(squared_diff)
mse
View(squared_diff)
y_pred <- predict(multinom, as.matrix(X_test))
squared_diff <- (y_test - y_pred)^2
data_frame <- data.frame(y_test,y_pred)
y_pred <- predict(multinom, as.matrix(X_test))
squared_diff <- (y_test - y_pred)^2
data_frame <- data.frame(y_test,y_pred)
data_frame
y_pred <- predict(multinom, as.matrix(X_test))
data_frame <- data.frame(y_test,y_pred)
data_frame
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 50,
validation_split = 0.1,
verbose = 1)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 5,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
data_frame <- data.frame(y_test,y_pred)
data_frame
y_pred <- predict(multinom, as.matrix(X_test))
squared_diff <- (y_pred - y_test)^2
# Calculate the Mean Squared Error (MSE)
mse <- mean(squared_diff)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 10,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
linreg_formula = score~.
predict_linreg = lm(formula, train = train, test = val)
set.seed( 5 )
data_sets = partition( data = train_data, prob = c( 0.8, 0.2 ) )
set.seed( 5 )
data_sets = partition( data = train_data, prob = c( 0.8, 0.2 ) )
set.seed( 5 )
data_sets = partition( data = train_data, prob = c( 0.8, 0.2 ) )
packages.install(partition)
install.packages('partition')
#| label: R packages
#| echo: false
#| warning: false
#| message: false
library(tidyverse)
library(dummy)
library(ISLR)
library(partition)
#library( ggplot2 )
# additional packages here
set.seed( 5 )
data_sets = partition( data = train_data, prob = c( 0.8, 0.2 ) )
linreg_formula = score~.
predict_linreg = lm(formula, train = X_train, test = y_train)
predict_linreg = lm(score~., train = X_train, y_train)
predict_linreg = lm(score~X_train., train = X_train, y_train)
predict_linreg = lm(score~.X_train, train = X_train, y_train)
y_Train
y_Train
y_Train
predict_linreg = lm(y_train ~.,data=X_train)
predict_linreg = lm(data.frame(y_train) ~.,data=X_train)
predict_linreg = lm(score~.,data=X_train)
predict_linreg = lm(Ã§score~.,data=X_train)
predict_linreg = lm(score~.,data=train_data_dummy)
summary(lin_reg)
predict_linreg = lm(score~.,data=train_data_dummy)
summary(predict_linreg)
y_pred_lin_reg <- predict(lin_reg)
predict_linreg = lm(score~.,data=train_data_dummy)
summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg)
mse(train_data$score, predict(predict_linreg))
predict_linreg = lm(score~.,data=train_data_dummy)
summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg)
mse(train_data_dummy$score, predict(predict_linreg))
train_data_dummy <- dummy_cols(train_data,remove_first_dummy=TRUE,remove_selected_columns=TRUE)
train_data_dummy
test_data_dummy <- dummy_cols(test_data,remove_first_dummy=TRUE,remove_selected_columns=TRUE)
test_data_dummy
predict_linreg = lm(score~.,data=train_data_dummy)
summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg)
mse(train_data_dummy$score, predict(predict_linreg,newdata = test_data_dummy))
predict_linreg = lm(score~.,data=train_data_dummy)
summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg)
y_pred_lin_reg
predict_linreg = lm(score~.,data=train_data_dummy)
summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg,new_data=test_data_dummy)
y_pred_lin_reg
predict_linreg = lm(score~.,data=train_data_dummy)
#summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg,new_data=test_data_dummy)
summary(y_pred_lin_reg)
predict_linreg = lm(score~.,data=train_data_dummy)
#summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg,new_data=test_data_dummy)
y_pred_lin_reg
predict_linreg = lm(score~.,data=train_data_dummy)
#summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg,new_data=test_data_dummy)
# Calculate the Mean Squared Error (MSE)
mean((y_pred_lin_reg - y_test)^2)
predict_linreg = lm(score~.,data=train_data_dummy)
#summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg,new_data=test_data_dummy)
# Calculate the Mean Squared Error (MSE)
mean((y_pred_lin_reg - y_test)^2)
predict_linreg = lm(score~.,data=train)
#summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg,new_data=test)
# Calculate the Mean Squared Error (MSE)
mean((y_pred_lin_reg - y_test)^2)
predict_linreg = lm(score~.,data=train)
#summary(predict_linreg)
y_pred_lin_reg <- predict(predict_linreg,newdata=test)
# Calculate the Mean Squared Error (MSE)
mean((y_pred_lin_reg - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_batch_normalization() %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_batch_normalization() %>%
layer_dense(32, activation = 'relu') %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_batch_normalization() %>%
layer_dense(32, activation = 'relu') %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(4, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_batch_normalization() %>%
layer_dense(39, activation = 'relu') %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_batch_normalization() %>%
layer_dense(39, activation = 'relu') %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
batch_size = 5,
validation_split = 0.2,
verbose = 1)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
batch_size = 2,
validation_split = 0.2,
verbose = 1)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
batch_size = 20,
validation_split = 0.2,
verbose = 1)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
batch_size = 30,
validation_split = 0.2,
verbose = 1)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.2,
verbose = 1)
multinom <- keras_model_sequential() %>%
layer_dense(39, activation = 'relu') %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.2,
verbose = 1)
multinom <- keras_model_sequential() %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.2,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.05,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_dense(2, activation = 'relu') %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.05,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 20,
validation_split = 0.05,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 50,
validation_split = 0.05,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_dense(16, activation = 'relu') %>%
layer_dropout(0.2) %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam()
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 100,
validation_split = 0.1,
verbose = 1)
multinom <- keras_model_sequential() %>%
layer_dense(16, activation = 'relu') %>%
layer_dropout(0.05) %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam(0.0001)
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 100,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 100,
batch_size = 50,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_dense(16, activation = 'relu') %>%
layer_dropout(0.05) %>%
layer_dense(8, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam(0.0005)
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 50,
batch_size = 50,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_dense(16, activation = 'relu') %>%
layer_dropout(0.05) %>%
layer_dense(8, activation = 'relu') %>%
layer_dropout(0.05) %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam(0.0005)
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 50,
batch_size = 50,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
multinom <- keras_model_sequential() %>%
layer_dense(16, activation = 'relu') %>%
layer_dropout(0.05) %>%
layer_dense(16, activation = 'relu') %>%
layer_dense(1)
multinom %>% compile(
loss = 'mean_absolute_error',
optimizer = optimizer_adam(0.0005)
)
pl <- multinom %>% fit(as.matrix(X_train),
as.matrix(y_train),
epochs = 50,
batch_size = 50,
validation_split = 0.1,
verbose = 1)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
ggplot(data=NULL, aes(x=y_test,y=y_pred)) +
geom_point()
ggplot(data=NULL, aes(x=y_test,y=y_pred)) +
geom_point()+
geom_abline(intercept = 0, slope = 1, color = "blue") +
labs(x = "Observed", y = "Predicted") +
ggtitle("Observed vs. Predicted")
ggplot(data=NULL, aes(x=y_test,y=y_pred)) +
geom_point()+
geom_abline(intercept = 0, slope = 1, color = "blue") +
labs(x = "Observed", y = "Predicted") +
ggtitle("Observed vs. Predicted") +
xlim(c(-2,2))
ggplot(data=NULL, aes(x=y_test,y=y_pred)) +
geom_point()+
geom_abline(intercept = 0, slope = 1, color = "blue") +
labs(x = "Observed", y = "Predicted") +
ggtitle("Observed vs. Predicted") +
xlim(c(-2,2)) +
ylim(c(-2,2))
ggplot(data=NULL, aes(x=y_test,y=y_pred_lin_reg)) +
geom_point()+
geom_abline(intercept = 0, slope = 1, color = "blue") +
labs(x = "Observed", y = "Predicted") +
ggtitle("Observed vs. Predicted") +
xlim(c(-2,2)) +
ylim(c(-2,2))
y_pred <- predict(multinom, newdata=as.matrix(X_test))
y_pred <- predict(multinom, new_data=as.matrix(X_test))
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
y_pred <- predict(multinom, as.matrix(X_train))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_train)^2)
y_pred <- predict(multinom, as.matrix(X_train))
# Calculate the Mean Squared Error (MSE)
mean((y_train - y_pred)^2)
y_pred <- predict(multinom, as.matrix(X_train))
# Calculate the Mean Squared Error (MSE)
mean((as.matrix(y_train) - y_pred)^2)
y_pred <- predict(multinom, as.matrix(X_test))
# Calculate the Mean Squared Error (MSE)
mean((y_pred - y_test)^2)
y_pred <- predict(multinom, as.matrix(X_train))
# Calculate the Mean Squared Error (MSE)
mean((as.matrix(y_train) - y_pred)^2)

---
title: "Assignment3: Supervised Learning Competition - Group 11"
authors: 
  - Lourenço Santos Moitinho de Almeida
  - Liam Thomassen
  - Tim Rentenaar
  - Erdem Koçer 
date: last-modified
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false
# install.packages("brms")
# install.packages("glmnet")
# install.packages("xgboost")
# install.packages("mgcv")
# install.packages("e1071")
#library(keras)
#install_tensorflow()

library(tidyverse)
library(fastDummies)
library(ISLR)

library(corrplot)
library(ggplot2 )  
library(dplyr)
library(xgboost)
library("brms")
library(glmnet)
library(mgcv)
library(e1071)
library(caret)
library(tensorflow)
library(keras)

## Run install_tensorflow if keras gives an error
```

```{r}
#| label: Data loading
#| echo: false
test_data <- readRDS("test.rds")
train_data <-readRDS("train.rds")
```

## Data Description & Data Exploration

In order to predict student performance using the various characteristics of the students, we begin our analysis by exploring our dataset. Our exploratory data analysis will roughly follow the guidelines stated by Tukey and Peng. With that said, in order to get a preliminary understanding of our data, we ran the `str()` command as shown below in *Table 1*:

**Table 1:** Checking the Packaging of the Dataset using `str()`

```{r}
#| label: Checking Packaging 
str(train_data)
```

```{r}
#| label: Splitting the training dataset into variable types 
factor_cols <- names(train_data)[sapply(train_data, is.factor)]
num_cols <- names(train_data)[sapply(train_data, is.numeric)]
```

This training dataset contains `r nrow(train_data)` rows and `r ncol(train_data)` columns. Furthermore, the column `score` represents our dependent variable while the rest of the columns are explanatory variables. This dataset contains two types of variables: factor (categorial) and numerical variables. Each column can be categorized into one of the two variables types:

-   <u>Factor Variables:</u> `r paste(factor_cols, collapse=", ")`
-   <u>Numerical Variables:</u> `r paste(num_cols, collapse=", ")`

Additionally, *Figure 1* dispalys that this dataset doesn't contain any missing values. Therefore, any missing information imputation strategies won't be required during the data (pre)processing step of our analysis.

**Figure 1:** Counts of all the Missing Data per Column

```{r}
#| label: Missing data counts per column 
sapply(train_data, function(x) sum(is.na(x)))
```

An important thing to note is that despite the insights given by running the `str()` command, reading the column descriptions file gives contradicting information. For instance out of the `r sum(sapply(train_data, is.numeric))` numeric columns, only `age`, `absences` and `score` are 'true' numericcal variables. The rest of the variables are ordinal variables such as `Dalc`, a variable ranging from 1-5 based on workday alcohol consumption. This assessment is further backed up by analysing the top and bottom of the dataset as shown below in *Table 2* and *Table 3*:

**Table 2:** Top of the Dataset

```{r}
#| label: Top of the Dataset 
head(train_data)
```

**Table 3:** Bottom of the Dataset

```{r}
#| label: Bottom of the Dataset 
tail(train_data)
```

### The Analysis for the Dependent Variable: Score

```{r}
#| label: 5-number summary for score
minimum_value <- round(min(train_data$score), 2)
maximum_value <- round(max(train_data$score), 2)
median_value <- round(median(train_data$score), 2)
q1_value <- round(quantile(train_data$score, 0.25), 2)
q3_value <- round(quantile(train_data$score, 0.75), 2)
```

The whole aim of this analysis is to predict scores for students in the test dataset. In order to do so we took a closer look at the variable `score`. Below, *Figure 2* displays the spread of `score` in addition to a 5-number summary. Following the inter-quartile range rule for outlier detection, scores outside the range of $`r q1_value-1.5*(q3_value-q1_value)`\leq x \leq `r q3_value+1.5*(q3_value-q1_value)`$ would be considered outliers.

**Figure 2:** The Spread and 5-Number Summary of Score

```{r}
ggplot(train_data, aes(x = score)) +
  geom_density(fill = "skyblue") +
  xlab("Scores") +
  ylab("Frequency") +
  ggtitle("Density Plot of Scores with 5-Number Summary Overlay") +
  theme(plot.title = element_text(hjust = 0.5)) +
  
  geom_vline(aes(xintercept = minimum_value, color = "Minimum = -2.71"), linetype = "longdash") + 
  annotate(geom = "text", label = "Minimum", color = "black", angle = 90, x = minimum_value-0.1, y = 0.2, size = 3) +
  
  geom_vline(aes(xintercept = maximum_value, color = "Maximum = 2.23"), linetype = "longdash") + 
  annotate(geom = "text", label = "Maximum", color = "black", angle = 90, x = maximum_value-0.1, y = 0.2, size = 3) +
  
  geom_vline(aes(xintercept = median_value, color = "Median = -0.03"), linetype = "longdash") + 
  annotate(geom = "text", label = "Median", color = "black", angle = 90, x = median_value-0.1, y = 0.2, size = 3) +
  
  geom_vline(aes(xintercept = q1_value, color = "Quartile 1 = -0.63"), linetype = "longdash") + 
  annotate(geom = "text", label = "Quartile 1", color = "black", angle = 90, x = q1_value-0.1, y = 0.2, size = 3) +
  
  geom_vline(aes(xintercept = q3_value, color = "Quartile 3 = 0.64"), linetype = "longdash") + 
  annotate(geom = "text", label = "Quartile 3", color = "black", angle = 90, x = q3_value-0.1, y = 0.2, size = 3) +
  
  scale_color_manual("5-Number Summary", values = c("Minimum = -2.71" = "black", "Maximum = 2.23" = "black", "Median = -0.03" = "black", "Quartile 1 = -0.63" = "black", "Quartile 3 = 0.64" = "black"))

```

### Correlation between Score and the Explanatory Variables

For the correlation between `score` and the rest of the explanatory variables we divided the training dataset by variable type. *Table 4* displays the correlation between the numerical variables. The variables `failures`, `goout` and `age` measuring the student's number of past class failures, time spent with friend and age seemed to have the greatest negative correlation against `score` while `Medu` and `Fedu`, variables measuring their parent's education seemed to have the greatest positive correlation. Additionally, `goout` and `Dalc` (workday alcohol consumption) are both positively correlated with `Walc` (weekend alcohol consumption). In contrast, *Table 5* displays the correlation between the converted dummy categorical variables and `score`. Although, none of the categorical variables seemed to be correlated with `score`. The only strongly correlated variables in this table were between the categories of `Mjob` and `Fjob` (variables that measure the parent's occupation).

**Table 4:** Correlation Matrix between Numerical Variables and Score

```{r, fig.width = 10, fig.height = 10}
#| label: Correlation between numerical variables and score
corrplot(cor(train_data[sapply(train_data, is.numeric)]), diag = FALSE)
```

**Table 5:** Correlation Matrix between Converted Categorical Variables and Score

```{r, fig.width = 10, fig.height = 10}
#| label: Correlation between dummy categorical variables and score
temp_df <- train_data[, !sapply(train_data, is.numeric)]
temp_df_dummy <- dummy_cols(temp_df,remove_first_dummy=TRUE,remove_selected_columns=TRUE)
temp_df_dummy$score <- train_data$score

corrplot(cor(temp_df_dummy), diag = FALSE)
```

## Model Descriptions 

For our analysis we implemented five fundamental supervised learning algorithms that encompassed nine different models in total.

**0. Trivial Baseline - Mean** 

Even though we don't consider this technique a model we included a trivial baseline in order to compare our model's results with a basic prediction. In our case we decided to take the mean `score` from the training data and apply it to the dev data. 

**1. K-Nearest Neighbors (KNN)**
- <u>1.1 KNN:</u> 
K-Nearest Neighbors is a non-paramtric algorithm that is mainly aimed towards classification however it can also be applied towards regression tasks. Similarly to the trivial baseline, we expected that the KNN model would under perform in comparison to the other models explained below. Despite this reasoning, we decided to include the model for comparison evlauations in addition to the relatively easy implementation of it. 

**2. Regression**
- <u>2.1 Linear Regression:</u> 
Similarly to the KNN model, we chose to include a linear regression model in order to compare and evaluate our more complex models against it. 

- <u>2.2 Lasso Regression:</u>
Lasso regression is very similar to linear regression. The difference between the two models is that the lasso model includes regularization and feature selection. This model decreases coefficients towards zero which is ideal for datasets with an increased amount of explanatory variables. The regularization technique aims to decrease the likelihood of over fitting the model.

- <u>2.3 Generalized Additive Models (GAM):</u>
In contrast to the linear and lasso regression models above, the GAM model aims to analyze linear and non-linear relationships. Due to the variation of data types in our dataset, the GAM model allows for more flexibility at the cost of increasing model complexity. 

**3. Neural Networks**
- <u>3.1 and 3.2 Simple and Complex Neural Network:</u>
Neural networks are complex machine learning algorthims that are able to be implemented towards more complex tasks such as modeling non-linear relationships and image recognition. Although one of the advantages of this model stems from its versatility and scalabilty, it comes at a cost of increasing the computation cost and over fitting the data. For our analysis, we implemented two neural networks that differed in complexity. The main difference between the two models used were the amount of activation layers included in the model. 

**4. Ensemble Methods**
- <u>4.1 XGBoost:</u>
XGBoost is an ensemble methods of 'weak learners' that implements gradient boosting. This tree-based model trains decision trees on different subsets of the data that aim to improve the residual of the model. In order to get a prediction, XGBoost combines this set of 'weak learners' in order to improve it prediction capabilities. 

- <u>4.2 Mixed Model - XGBoost and Lasso Regression</u>
Unlike the previous models discussed, the mixed model combines the lasso and XGBoost model together rather than implementing a new algorithm. The aim of this model is to improve prediction capability through the regularization used in lasso regression in conjunction to XGBoost's ability of combining sets of weak learners. This combination implicitly implements feature selection due to the fact that lasso regularization decreases the weights of each coefficient while the tree-based model creates subsets of 'weak learners' that correct based on the results from the previous decision tree in the sequence. 

**5. Support Vector Machine (SVM)**
- <u>5.1 SVM</u>
The SVM model (or SVR for regression) aims to analyze the data and create some type of model that slightly deviates from the actual data. The prediction created by this model aim to fall within a certain threshold. Much like the previous models discussed, this model is able to analyze non-linear relationships and is less sensitive towards outliers. 

## Data Transformation and Pre-processing 
### Data Transformation 
From our data analysis and exploration, we identified that the data set contained two different types of variables: Factor (categorial) and Numerical. The numerical variables consisted of 'true' numerical variables and ordinal variables (based on the variable descriptions document). Fortunately, the survey student data responses were collected such that ordinal encoding wasn't required. On other other hand, data transformation was required for the categorical variables. We converted all these variables by transforming them into dummy variables. Additionally, for each variable we left one dummy column out in order to avoid multi-collinearity. 

```{r}
#| label: dummy encoding the training and test dataset 
train_dummy <- dummy_cols(train_data, remove_selected_columns = TRUE)
test_dummy <- dummy_cols(test_data, remove_selected_columns = TRUE)
```

### Data Reduction
After dummy encoding the categorical variables in our dataset we decided not to remove any explanatory variables from our prediction model equations. Instead, we chose to implement machine learning models that excelled at handing high dimensional datasets such as Lasso regression and the GAM model. Furthermore, based on our correlation analysis, we concluded that data reduction wasn't required either. 

### Data Cleaning 
From the information gathered from exploring the dataset, we didn't identify any missing data. Therefore, missing data imputation strategies were not used for our analysis. Furthermore, we didn't find any significant 'noisy' or outlier data points in our dataset. Additionally, the regression models handle these potential issues by smoothing the data into regression functions 

## Model Exploration and Comparison
### Model Training 
- Creation of the K-fold Cross Validation:

```{r}
#| label: K-fold cross validation
train_control <- trainControl(method="cv", number=5)
```

- Creation of the Train/Dev 80-20 Random Split: 

```{r}
#| label: train/dev 80-20 random split
sample_size <- floor(0.8 * nrow(train_dummy))

set.seed(1)
train_index <- sample(seq_len(nrow(train_dummy)), size=sample_size)

train_dummy_train <- train_dummy[train_index,]
train_dummy_dev <- train_dummy[-train_index,]
```

### Model Implementation 
For our model we decided to use the techniques of K-fold cross validation and a train/dev split. For K-fold cross validation we implemented it on the models of linear regression, a neural network and KNN. As for the train/dev split we decided to implement it on lasso regression, XGBoost, SVM, GAM, the mixed model and on a simple and more complex neural network. 

#### Model Implementation using K-fold Cross Validation 
**K-Nearest Neighbors (KNN)**

```{r}
# training knn using k-fold crossvalidation
#knn <- train(score ~ ., data=train_dummy, method="knn", trControl=train_control)
```

```{r}
#print(knn)
```


**Linear Regression**
```{r}
# training a linear model using k-fold crossvalidation
#lm <- train(score ~ ., data=train_dummy, method="lm", trControl=train_control)
```

```{r}
# resulting scores (including MAE) from linear regression
#print(lm)
```


**Neural Networks (using k-fold)**
```{r}
#| warning: false
#| results: hide
# training a model averaged neural network using k-fold crossvalidation
#nn <- train(score ~ ., data=train_dummy, method="avNNet", trControl=train_control)
```

```{r}
#print(nn)
```

#### Model Implementation using a Train/Dev Split
```{r}
X_train <- train_dummy_train[, -which(names(train_dummy_train) == "score")]
y_train <- data.frame(train_dummy_train$score)
X_test <- train_dummy_dev[, -which(names(train_dummy_dev) == "score")]
y_test <- train_dummy_dev$score
```

**Trivial Baseline - Mean**
```{r}
# creating a mean trivial baseline is not possible with the caret package
# the mean trivial baseline will be created with a single train/dev split

mean_score_train <- mean(train_dummy_train$score)
```
```{r}
# resulting MAE and MSE from the trivial baseline
mae <- mean(abs(train_dummy_dev$score - mean_score_train))
mse <- mean((train_dummy_dev$score - mean_score_train) ** 2)

print(mae)
```
```{r}
print(mse)
```

**Lasso Regression**
```{r}
original_weights <- rep(1, nrow(X_train)) 

cv_model <- cv.glmnet(data.matrix(X_train), data.matrix(y_train), alpha = 1)

best_lambda <- cv_model$lambda.min
best_model <- glmnet(data.matrix(X_train), data.matrix(y_train), alpha = 1, lambda = best_lambda)
plot(cv_model) 
```

```{r}
pred_y_lasso <- predict(best_model, s = best_lambda, newx = data.matrix(X_test))
cat("mse of lasso model: ", mean((as.matrix(pred_y_lasso)-as.matrix(y_test))^2))
```

**Generalized Additive Models (GAM)**
```{r}
gam_model <- gam(score ~ s(age,k=3) + 
                   s(failures, k=4) + 
                   s(studytime, k = 4) +
                   internet_no +
                   sex_M +
                   famsize_GT3 +
                   Dalc +
                   Walc +
                   Pstatus_A +
                   reason_reputation, 
                 data = train_dummy_train)

testing_data <- data.frame(X_test)

pred_y_gam <- predict(gam_model, newdata = testing_data)
cat("mse of GAM model: ", mean((as.matrix(pred_y_gam)-as.matrix(y_test))^2))
```

**Simple Neural Network (using Train/Dev)**
```{r}
model <- keras_model_sequential() %>%
    layer_dense(16, activation = 'relu') %>%
    layer_dropout(0.1) %>%
    layer_dense(8, activation = 'relu') %>%
    layer_dropout(0.1) %>%
    layer_dense(1)

model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adam()
)

plot_model <- model %>% fit(as.matrix(X_train), 
      as.matrix(y_train), 
      epochs = 100, 
      validation_split = 0.1,  
      verbose = 0)

pred_y_nn <- predict(model, as.matrix(X_test))
```

```{r}
plot(plot_model)
```

```{r}
cat("mse of simple NN model: ", mean((as.matrix(pred_y_nn)-as.matrix(y_test))^2))
```


**Complex Neural Network (using Train/Dev)**
```{r}
model_complex <- keras_model_sequential() %>%
    layer_dense(512, activation = 'relu', regularizer_l1_l2(l1=0.01,l2=0.01)) %>%
    layer_dropout(0.2) %>%
    layer_dense(128, activation = 'relu', regularizer_l1_l2(l1=0.01,l2=0.01)) %>%
    layer_dropout(0.2) %>%
    layer_dense(64, activation = 'relu', regularizer_l1_l2(l1=0.01,l2=0.01)) %>%
    layer_dropout(0.2) %>%
    layer_dense(32, activation = 'relu', regularizer_l1_l2(l1=0.01,l2=0.01)) %>%
    layer_dropout(0.2) %>%
    layer_dense(16, activation = 'relu', regularizer_l1_l2(l1=0.01,l2=0.01)) %>%
    layer_dropout(0.2) %>%
    layer_dense(8, activation = 'relu') %>%
    layer_dense(1)

model_complex %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adam()
)

plot_model <- model_complex %>% fit(as.matrix(X_train), 
      as.matrix(y_train), 
      epochs = 85, 
      validation_split = 0.1,  
      verbose = 0)

pred_y_nn_complex <- predict(model_complex, as.matrix(X_test))
```

```{r}
plot(plot_model)
```

```{r}
cat("mse of complex NN model: ", mean((as.matrix(pred_y_nn_complex)-as.matrix(y_test))^2))
```



**XGBoost**
```{r}
xgb_train = xgb.DMatrix(data = as.matrix(X_train), label = as.matrix(y_train))
xgb_test = xgb.DMatrix(data = as.matrix(X_test), label = as.matrix(y_test))

watchlist = list(train=xgb_train, test=xgb_test)
xgb_model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 10,verbose=1)
```
```{r}
model_xgboost = xgboost(data = xgb_train, max.depth = 3, nrounds = 5, verbose = 0)
pred_y_xgboost <- predict(model_xgboost,xgb_test)

cat("mse of xgboost model: ", mean((as.matrix(pred_y_xgboost)-as.matrix(y_test))^2))
```


**Mixed Model - XGBoost and Lasso Regression**
```{r}
combined_model <- (pred_y_xgboost + pred_y_lasso) / 2

cat("mse of combined model: ", mean((as.matrix(combined_model)-as.matrix(y_test))^2))
```

**Support Vector Machine (SVM)**
```{r}
svm_model <- svm(x = X_train, y = y_train, kernel = "radial", cost = 1)
pred_y_svm <- predict(svm_model, newdata = X_test)
cat("mse of SVM model: ", mean((as.matrix(pred_y_svm)-as.matrix(y_test))^2))
```
### Model Comparison 
For the comparison of the models we decided to compare the MSE values. 
- <u>K-Fold Cross Validation Comparison:</u>

According to the cross validation results below in *Table 5*, the best-performing model was KNN at an optimal k value of 9. However, these are quick calculations to see what kind of models would work for this problem. That is, by tuning the hyperparameters of more complex models such as neural networks or tree-based algorithms, better scores on the train set can be achieved.

**Table 6:** K-Fold MSE Comparison
```{r}
data.frame(
  model       = c("KNN (k=9)", "Linear Regression", "Neural Netowrk (using K-fold"),
  MSE = c(0.851,0.886 ,0.915)
)
```

- <u>Train/Dev Split Comparison:</u>
Below, *Table 7* compares all the MSE values of all the models that used the train/dev split. Out of these models, the mixed model that combined XGBoost and Lasso Regression had the lowest MSE of 0.684. The model reduced the MSE value by 0.064 from the trivial baseline. Furthermore, it is interesting to highlight that all the models that used the train/dev split performed noticibly better than the models using the K-fold cross validation strategy. 

**Table 7:** Train/Dev MSE Comparison
```{r}
data.frame(
  model       = c("Trivial Baseline - Mean", "Lasso Regression", "GAM", "Simple Neural Netowrk (using Train/Dev)", "Complex Neural Netowrk (using Train/Dev)", "XGBoost", "Mixed Model", "SVM" ),
  MSE = c(0.748, 0.715, 0.724, 0.758, 0.767, 0.696, 0.684,0.783)
)
```

## Conclusion and Chosen Model

*The current best approach involves an ensemble of two powerful models: XGBoost and Lasso Regression.*

*Neural Networks were not as effective in this context due to the high number of features and a relatively small sample size. In contrast, models like Lasso Regression implement feature selection, allowing them to capture the underlying data structures more effectively without the risk of overfitting.*

*However, it's essential to acknowledge that the confidence interval of the Lasso Regression results makes it challenging to accurately predict the final error rate for unseen samples. In other words, we should anticipate the Lasso model's performance to fall within this confidence interval when tested. When it comes to actual predictions, it's common for the model to perform less favorably than expected.*

Another compelling approach explored in this study was the use of Generalized Additive Models (GAM), representing an extension of traditional linear regression models specifically tailored to unveil nonlinear relationships within the dataset. For this experiment, the GAM model was employed with parameters derived from the Lasso model. The outcomes proved to be slightly superior to alternative models but still fell short of the exceptional performance achieved by XGBoost. Notably, it's intriguing to observe that the GAM model demonstrated the capability to outperform Neural Networks, despite operating with a substantially reduced feature set.

## Result

*Apply the chosen model to predict the scores for the test data and export the result*

```{r}
xgb_test_Data <- xgb.DMatrix(data = as.matrix(test_dummy))
pred_real_y_xgboost <- predict(model_xgboost, xgb_test_Data)

pred_real_y_lasso <- predict(best_model, s = best_lambda, newx = data.matrix(test_dummy))

combined_real_model <- (pred_real_y_lasso + pred_real_y_xgboost) / 2

new_frame <- data.frame(pred_real_y_xgboost,pred_real_y_lasso,combined_real_model)

ggplot(data = new_frame) +
  geom_density(aes(pred_real_y_xgboost, fill = "XGboost", alpha=0.2)) +
  geom_density(aes(s1, fill = "Lasso", alpha=0.2)) +
  geom_density(aes(s1.1, fill = "Combined Model", alpha=0.2)) +
  xlab("Scores") +
  ylab("Frequency") +
  ggtitle("Results from two models") +
  theme(plot.title = element_text(hjust = 0.5))

```

### Save the file

```{r}
saveRDS(combined_real_model, "predictions.rds")
```

## Team member contributions

-   Lourenço Santos Moitinho de Almeida: Data Description & Data Exploration, Report
-   Liam Thomassen: Data Description & Data Exploration
-   Tim Rentenaar: Data Preparation, Model Exploration (Trivial baseline, cross validated models)
-   Erdem Koçer: Model Exploration (Other models trained with a single train/dev split), Conclusion and Chosen Mode, Results

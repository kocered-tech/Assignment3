---
title: "FinalModels"
format: html
editor: visual
---

## Libraries

```{r}
library(tidyverse)
library(keras)
library(ggplot2)
library(xgboost)
library(fastDummies)
library(dplyr)
library(caret)
library(corrplot)
library(glmnet)
library(ROSE)
library(mgcv)
library("e1071")
library("brms")
```

## Get Data

```{r}
train_data <- readRDS("train.rds")
```

```{r}
train_data_dummy <- dummy_cols(train_data,remove_first_dummy=TRUE,remove_selected_columns=TRUE)

sample <- sample(c(TRUE, FALSE), nrow(train_data_dummy), replace=TRUE, prob=c(0.80,0.2))
train  <- train_data_dummy[sample, ]
test   <- train_data_dummy[!sample, ]
```

```{r}
X_train <- train[, -which(names(train) == "score")]
y_train <- data.frame(train$score)
X_test <- test[, -which(names(train) == "score")]
y_test <- test$score
```

```{r}
get_random_Data <- function() {
  train_data <- readRDS("train.rds")
  train_data_dummy <- dummy_cols(train_data,remove_selected_columns   =TRUE)

  sample <- sample(c(TRUE, FALSE), nrow(train_data_dummy), replace=TRUE, prob=c(0.9,0.1))
  train  <- train_data_dummy[sample, ]
  test   <- train_data_dummy[!sample, ]
  
  X_train <- train[, -which(names(train) == "score")]
  y_train <- data.frame(train$score)
  X_test <- test[, -which(names(train) == "score")]
  y_test <- test$score
  
  return(list(X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test))
}

```

```{r}
data = get_random_Data()
```

## Lasso

```{r}
#perform k-fold cross-validation to find optimal lambda value
original_weights <- rep(1, nrow(X_train))  # Initialize all weights to 1
score_threshold <- 1  # Define the threshold

cv_model <- cv.glmnet(data.matrix(X_train), data.matrix(y_train), alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_model <- glmnet(data.matrix(X_train), data.matrix(y_train), alpha = 1, lambda = best_lambda)
plot(cv_model) 
```

```{r}
pred_y_lasso <- predict(best_model, s = best_lambda, newx = data.matrix(X_test))
lasso <- mean((y_test - pred_y_lasso)^2)
lasso
```

```{r}
xgb_train = xgb.DMatrix(data = as.matrix(X_train), label = as.matrix(y_train))
xgb_test = xgb.DMatrix(data = as.matrix(X_test), label = as.matrix(y_test))

watchlist = list(train=xgb_train, test=xgb_test)
xgb_model = xgb.train(data = xgb_train, max.depth = 6, watchlist=watchlist, nrounds = 25,verbose=1)
```

```{r}
model_xgboost = xgboost(data = xgb_train, max.depth = 6, nrounds = 25, verbose = 0)
pred_y_xgboost <- predict(model_xgboost,xgb_test)

# Calculate the Mean Squared Error (MSE)
mean((pred_y_xgboost - y_test)^2)
```

```{r}
mod_lm = gam(score ~ failures, data = train)
summary(mod_lm)
```

```{r}
pred_gam <- predict(mod_lm,newdata=X_test)
mean((pred_gam - y_test)^2)
```

```{r}
model <- gam(score ~.,
            data = train)

pred_gam <- predict(model,newdata=X_test)
mean((pred_gam - y_test)^2)
```

```{r}
# Fit a Bayesian regression model with a Gaussian prior
weights <- c(1, 0.3, 0.3) 

combined_prior <- prior(

    normal(0, 0.5),
  class = "b"
)

model <- brm(score ~.,data = train, chains = 4, iter = 8000, prior = combined_prior)

# View the model summary
summary(model)
```

```{r}
predicted_values <- predict(model,newdata = test)
predicted_values<-data.frame(predicted_values)
predicted_values<- predicted_values$Estimate
mean((as.matrix(predicted_values)-as.matrix(y_test))^2)
```

```{r}
summary(svm_model)
```

```{r}
multinom <- keras_model_sequential() %>%
  layer_dense(256) %>%
  layer_dropout(0.2) %>%
  layer_dense(128,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(64,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(32,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(16,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(8,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(1)
```

```{r}
multinom <- keras_model_sequential() %>%
  layer_dense(256,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(128,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(64,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(32,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(16,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(8,activation="relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(1)

multinom %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam()
)

pl <- multinom %>% fit(as.matrix(X_train), 
      as.matrix(y_train), 
      epochs = 400, 
      validation_split = 0.2,  
      verbose = 1)


pred_y_nn <- predict(multinom, as.matrix(X_test))
mean <- mean((as.matrix(y_test) - pred_y_nn)^2)
plot(pl)
```

```{r}
model <- multinom
pred_y_nn <- predict(multinom, as.matrix(X_test))

# Calculate the Mean Squared Error (MSE)
mean((as.matrix(y_test) - pred_y_nn)^2)
```

```{r}
ggplot(data=NULL, aes(x=y_test,y=pred_y_nn)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue") +
  labs(x = "Observed", y = "Predicted") +
  ggtitle("Observed vs. Predicted") +
  xlim(c(-2,2)) +
  ylim(c(-2,2)) +
  coord_fixed(ratio = 1)

```

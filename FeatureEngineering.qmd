---
title: "Erdem_NN1"
format: html
editor: visual
---

## Install libraries

Uncomment to install libraries

```{r}
#install.packages("drat", repos="https://cran.rstudio.com")
#drat:::addRepo("dmlc")
#install.packages("xgboost")
#install.packages("caret)
#install.packages("xgboost)
#install.packages("corrplot")
#install.packages("glmnet")
```

```{r}
library(tidyverse)
library(keras)
library(ggplot2)
library(xgboost)
library(fastDummies)
library(dplyr)
library(caret)
library(corrplot)
library(glmnet)
```

## Data Preparation

```{r}
train_data = readRDS("train.rds")
train_data
```

## EDA

```{r}
train_data |> ggplot(aes(x=score)) +
  geom_histogram()
```

### Lasso Regression

```         
```

```{r}
plot(cv_model) 
```

```{r}

```

## Convert factors to dummy variables

```{r}
train_data_dummy <- dummy_cols(train_data,remove_first_dummy=TRUE,remove_selected_columns=TRUE)
train_data_dummy
```

```{r}
corrplot(cor(train_data_dummy)[1:20,1:20], method="color")
corrplot(cor(train_data_dummy)[20:40,20:40], method="color")
```

## Sample train and test data

```{r}
set.seed(5)
sample <- sample(c(TRUE, FALSE), nrow(train_data_dummy), replace=TRUE, prob=c(0.8,0.2))
train  <- train_data_dummy[sample, ]
test   <- train_data_dummy[!sample, ]
```

## Train, test split

```{r}
X_train <- train[, -which(names(train) == "score")]
y_train <- data.frame(train$score)
X_test <- test[, -which(names(train) == "score")]
y_test <- test$score
```

```{r}
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(data.matrix(X_train), data.matrix(y_train), alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_model <- glmnet(data.matrix(X_train), data.matrix(y_train), alpha = 1, lambda = best_lambda)
coef(best_model)
```

## Feature Selection

```{r}
features <- c("romantic_yes","famsup_yes","schoolsup_no","Fjob_other","Mjob_other","Mjob_health","sex_M","failures","studytime","health","traveltime","Medu","age")
```

```{r}
train_data_dummy <- dummy_cols(train_data,remove_first_dummy=TRUE,remove_selected_columns=TRUE)
train_data_dummy

train_data_dummy %>% select(features)

sample <- sample(c(TRUE, FALSE), nrow(train_data_dummy), replace=TRUE, prob=c(0.8,0.2))
train  <- train_data_dummy[sample, ]
test   <- train_data_dummy[!sample, ]
```

```{r}
X_train <- train[, -which(names(train) == "score")]
y_train <- data.frame(train$score)
X_test <- test[, -which(names(train) == "score")]
y_test <- test$score
```

## Models

## KNN

```{r}
knnmodel = knnreg(X_train, y_train$train.score,k=5)
pred_y_knn = predict(knnmodel, X_test)

mse = mean((y_test - pred_y_knn)^2)
mae = caret::MAE(y_test, pred_y_knn)
rmse = caret::RMSE(y_test, pred_y_knn)

cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmse)
```

```{r}
ggplot(data=NULL, aes(x=y_test,y=pred_y_knn)) +
  geom_point()+
  geom_abline(intercept = 0, slope = 1, color = "blue") +
  labs(x = "Observed", y = "Predicted") +
  ggtitle("Observed vs. Predicted") +
  xlim(c(-2,2)) +
  ylim(c(-2,2))
```

## XGBoost

```{r}
xgb_train = xgb.DMatrix(data = as.matrix(X_train), label = as.matrix(y_train))
xgb_test = xgb.DMatrix(data = as.matrix(X_test), label = as.matrix(y_test))

watchlist = list(train=xgb_train, test=xgb_test)
xgb_model = xgb.train(data = xgb_train, max.depth = 8, watchlist=watchlist, nrounds = 100)
```

```{r}
model_xgboost = xgboost(data = xgb_train, max.depth = 8, nrounds = 40, verbose = 0)
```

## XGBoost model performance

```{r}
pred_y_xgboost <- predict(model_xgboost,xgb_test)

# Calculate the Mean Squared Error (MSE)
mean((pred_y_xgboost - y_test)^2)
```

```{r}
ggplot(data=NULL, aes(x=y_test,y=pred_y_xgboost)) +
  geom_point()+
  geom_abline(intercept = 0, slope = 1, color = "blue") +
  labs(x = "Observed", y = "Predicted") +
  ggtitle("Observed vs. Predicted") +
  xlim(c(-2,2)) +
  ylim(c(-2,2))
```

## Neural Network

```{r}
multinom <- keras_model_sequential() %>%
    layer_dense(16, activation = 'relu') %>%
    layer_dense(8, activation = 'relu') %>%
    layer_dense(1)

multinom %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adam()
)

```

```         
```

```{r}
pl <- multinom %>% fit(as.matrix(X_train), 
      as.matrix(y_train), 
      epochs = 15, 
      validation_split = 0.05,       
      verbose = 1)
```

```{r}
pred_y_nn <- predict(multinom, as.matrix(X_test))

# Calculate the Mean Squared Error (MSE)
mean((as.matrix(y_test) - pred_y_nn)^2)
```

```{r}
ggplot(data=NULL, aes(x=y_test,y=pred_y_nn)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue") +
  labs(x = "Observed", y = "Predicted") +
  ggtitle("Observed vs. Predicted") +
  xlim(c(-2,2)) +
  ylim(c(-2,2))
```

```{r}
predict_linreg = lm(score~.,data=train)
#summary(predict_linreg)
pred_y_linreg <- predict(predict_linreg,newdata=test)

# Calculate the Mean Squared Error (MSE)
mean((pred_y_linreg - y_test)^2)
```

```{r}
ggplot(data=NULL, aes(x=y_test,y=pred_y_linreg)) +
  geom_point()+
  geom_abline(intercept = 0, slope = 1, color = "blue") +
  labs(x = "Observed", y = "Predicted") +
  ggtitle("Observed vs. Predicted") +
  xlim(c(-2,2)) +
  ylim(c(-2,2))
```

```{r}
lasso <- mean((y_test - pred_y_lasso)^2)
knn <- mean((y_test - pred_y_knn)^2)
xgboost <- mean((y_test - pred_y_xgboost)^2)
lin <- mean((y_test - pred_y_linreg)^2)
nn <- mean((as.matrix(y_test) - pred_y_nn)^2)

cat("knn: ", knn, 
    "xgboost: ", xgboost,
    "linear: ", lin,
    "lasso: ", lasso,
    "nn: ",nn)


```
